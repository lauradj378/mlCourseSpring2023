#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
import torchvision.models as models

training_data = datasets.MNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.MNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)
labels_map = {
    0: "0",
    1: "1",
    2: "2",
    3: "3",
    4: "4",
    5: "5",
    6: "6",
    7: "7",
    8: "8",
    9: "9",
}
figure = plt.figure(figsize=(8,8))
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()

learning_rate = 1e-3
batch_size = 64
epochs = 40
n_classes = 10
n_iter = 10

train_dataloader = DataLoader(training_data, batch_size,shuffle = True)
test_dataloader = DataLoader(test_data, batch_size,shuffle = True)

class LeNet5(nn.Module):
    def __init__(self, n_classes):
        super().__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size=(5,5), padding=2,padding_mode = 'reflect'),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size = (2,2),stride = 2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size=(5,5)),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size = (2,2),stride = 2))
        self.layer3 = nn.Sequential(
                      nn.LazyLinear(120),
                      nn.ReLU(),
                      nn.LazyLinear(84),
                      nn.ReLU(),
                      nn.LazyLinear(n_classes))
        
    def forward(self, x):
        y = self.layer1(x)
        y = self.layer2(y)
        y = torch.flatten(y,start_dim=1) #flattens indices 1,2,3 (leaves 0 alone)
        y = self.layer3(y)
        return y

model = LeNet5(n_classes)

loss_fcn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
            
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        pred = model(X)
        loss = loss_fn(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
    
loss_fcn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fcn, optimizer)
    test_loop(test_dataloader, model, loss_fcn)
print("Done!")

# =============================================================================
# Epoch 1
# -------------------------------
# loss: 2.295531  [   64/60000]
# loss: 2.320037  [ 6464/60000]
# loss: 2.296892  [12864/60000]
# loss: 2.296090  [19264/60000]
# loss: 2.320568  [25664/60000]
# loss: 2.308286  [32064/60000]
# loss: 2.298447  [38464/60000]
# loss: 2.315607  [44864/60000]
# loss: 2.294322  [51264/60000]
# loss: 2.306081  [57664/60000]
# Test Error: 
#  Accuracy: 10.1%, Avg loss: 2.303411 
# 
# Epoch 2
# -------------------------------
# loss: 2.294767  [   64/60000]
# loss: 2.315603  [ 6464/60000]
# loss: 2.300448  [12864/60000]
# loss: 2.315179  [19264/60000]
# loss: 2.282922  [25664/60000]
# loss: 2.315998  [32064/60000]
# loss: 2.282169  [38464/60000]
# loss: 2.306753  [44864/60000]
# loss: 2.296108  [51264/60000]
# loss: 2.313192  [57664/60000]
# Test Error: 
#  Accuracy: 10.1%, Avg loss: 2.302244 
# 
# Epoch 3
# -------------------------------
# loss: 2.294484  [   64/60000]
# loss: 2.304237  [ 6464/60000]
# loss: 2.300646  [12864/60000]
# loss: 2.307706  [19264/60000]
# loss: 2.296791  [25664/60000]
# loss: 2.320072  [32064/60000]
# loss: 2.297018  [38464/60000]
# loss: 2.298606  [44864/60000]
# loss: 2.280976  [51264/60000]
# loss: 2.290875  [57664/60000]
# Test Error: 
#  Accuracy: 15.6%, Avg loss: 2.300855 
# 
# Epoch 4
# -------------------------------
# loss: 2.295832  [   64/60000]
# loss: 2.287055  [ 6464/60000]
# loss: 2.294112  [12864/60000]
# loss: 2.310732  [19264/60000]
# loss: 2.290824  [25664/60000]
# loss: 2.306132  [32064/60000]
# loss: 2.297038  [38464/60000]
# loss: 2.290794  [44864/60000]
# loss: 2.297823  [51264/60000]
# loss: 2.292505  [57664/60000]
# Test Error: 
#  Accuracy: 11.4%, Avg loss: 2.299709 
# 
# Epoch 5
# -------------------------------
# loss: 2.294834  [   64/60000]
# loss: 2.311841  [ 6464/60000]
# loss: 2.293652  [12864/60000]
# loss: 2.307478  [19264/60000]
# loss: 2.296512  [25664/60000]
# loss: 2.304438  [32064/60000]
# loss: 2.294174  [38464/60000]
# loss: 2.297767  [44864/60000]
# loss: 2.294745  [51264/60000]
# loss: 2.299992  [57664/60000]
# Test Error: 
#  Accuracy: 11.3%, Avg loss: 2.298611 
# 
# Epoch 6
# -------------------------------
# loss: 2.300861  [   64/60000]
# loss: 2.304615  [ 6464/60000]
# loss: 2.306043  [12864/60000]
# loss: 2.290546  [19264/60000]
# loss: 2.307697  [25664/60000]
# loss: 2.294102  [32064/60000]
# loss: 2.294116  [38464/60000]
# loss: 2.289159  [44864/60000]
# loss: 2.296963  [51264/60000]
# loss: 2.308734  [57664/60000]
# Test Error: 
#  Accuracy: 11.3%, Avg loss: 2.297078 
# 
# Epoch 7
# -------------------------------
# loss: 2.289539  [   64/60000]
# loss: 2.285558  [ 6464/60000]
# loss: 2.292153  [12864/60000]
# loss: 2.296688  [19264/60000]
# loss: 2.293053  [25664/60000]
# loss: 2.286851  [32064/60000]
# loss: 2.282205  [38464/60000]
# loss: 2.307830  [44864/60000]
# loss: 2.288195  [51264/60000]
# loss: 2.305577  [57664/60000]
# Test Error: 
#  Accuracy: 11.3%, Avg loss: 2.295245 
#
# Epoch 8
# -------------------------------
# loss: 2.287364  [   64/60000]
# loss: 2.300500  [ 6464/60000]
# loss: 2.276473  [12864/60000]
# loss: 2.276765  [19264/60000]
# loss: 2.284071  [25664/60000]
# loss: 2.288833  [32064/60000]
# loss: 2.286343  [38464/60000]
# loss: 2.269016  [44864/60000]
# loss: 2.289037  [51264/60000]
# loss: 2.278883  [57664/60000]
# Test Error: 
#  Accuracy: 27.6%, Avg loss: 2.277947 
# 
# Epoch 9
# -------------------------------
# loss: 2.271651  [   64/60000]
# loss: 2.273159  [ 6464/60000]
# loss: 2.260552  [12864/60000]
# loss: 2.266156  [19264/60000]
# loss: 2.268868  [25664/60000]
# loss: 2.275095  [32064/60000]
# loss: 2.293245  [38464/60000]
# loss: 2.272885  [44864/60000]
# loss: 2.281621  [51264/60000]
# loss: 2.269282  [57664/60000]
# Test Error: 
#  Accuracy: 33.6%, Avg loss: 2.266268 
# 
# Epoch 10
# -------------------------------
# loss: 2.260203  [   64/60000]
# loss: 2.263999  [ 6464/60000]
# loss: 2.273305  [12864/60000]
# loss: 2.263146  [19264/60000]
# loss: 2.254209  [25664/60000]
# loss: 2.249011  [32064/60000]
# loss: 2.253271  [38464/60000]
# loss: 2.252237  [44864/60000]
# loss: 2.238425  [51264/60000]
# loss: 2.222259  [57664/60000]
# Test Error: 
#  Accuracy: 38.7%, Avg loss: 2.243997 
# 
# Epoch 11
# -------------------------------
# loss: 2.223190  [   64/60000]
# loss: 2.238712  [ 6464/60000]
# loss: 2.239046  [12864/60000]
# loss: 2.239893  [19264/60000]
# loss: 2.234672  [25664/60000]
# loss: 2.237376  [32064/60000]
# loss: 2.220723  [38464/60000]
# loss: 2.217318  [44864/60000]
# loss: 2.223401  [51264/60000]
# loss: 2.203487  [57664/60000]
# Test Error: 
#  Accuracy: 41.2%, Avg loss: 2.193028 
# 
# Epoch 12
# -------------------------------
# loss: 2.208491  [   64/60000]
# loss: 2.176263  [ 6464/60000]
# loss: 2.195285  [12864/60000]
# loss: 2.167149  [19264/60000]
# loss: 2.173480  [25664/60000]
# loss: 2.121878  [32064/60000]
# loss: 2.147066  [38464/60000]
# loss: 2.101190  [44864/60000]
# loss: 2.045508  [51264/60000]
# loss: 2.050591  [57664/60000]
# Test Error: 
#  Accuracy: 46.0%, Avg loss: 2.033873 
# 
# Epoch 13
# -------------------------------
# loss: 2.079977  [   64/60000]
# loss: 1.954895  [ 6464/60000]
# loss: 1.944294  [12864/60000]
# loss: 1.931134  [19264/60000]
# loss: 1.817010  [25664/60000]
# loss: 1.803963  [32064/60000]
# loss: 1.851154  [38464/60000]
# loss: 1.734879  [44864/60000]
# loss: 1.711939  [51264/60000]
# loss: 1.483038  [57664/60000]
# Test Error: 
#  Accuracy: 62.2%, Avg loss: 1.487086 
# 
# Epoch 14
# -------------------------------
# loss: 1.582241  [   64/60000]
# loss: 1.559323  [ 6464/60000]
# loss: 1.308930  [12864/60000]
# loss: 1.181097  [19264/60000]
# loss: 1.136847  [25664/60000]
# loss: 1.195191  [32064/60000]
# loss: 1.159276  [38464/60000]
# loss: 1.038467  [44864/60000]
# loss: 1.091415  [51264/60000]
# loss: 0.969320  [57664/60000]
# Test Error: 
#  Accuracy: 75.6%, Avg loss: 0.874437 
# 
# Epoch 15
# -------------------------------
# loss: 0.872489  [   64/60000]
# loss: 0.820524  [ 6464/60000]
# loss: 0.898686  [12864/60000]
# loss: 0.936138  [19264/60000]
# loss: 0.756794  [25664/60000]
# loss: 0.672810  [32064/60000]
# loss: 0.816770  [38464/60000]
# loss: 0.831048  [44864/60000]
# loss: 0.571656  [51264/60000]
# loss: 0.616907  [57664/60000]
# Test Error: 
#  Accuracy: 80.5%, Avg loss: 0.657001 
# 
# Epoch 16
# -------------------------------
# loss: 0.673886  [   64/60000]
# loss: 0.842599  [ 6464/60000]
# loss: 0.735154  [12864/60000]
# loss: 0.562429  [19264/60000]
# loss: 0.714908  [25664/60000]
# loss: 0.645182  [32064/60000]
# loss: 0.746931  [38464/60000]
# loss: 0.574794  [44864/60000]
# loss: 0.503609  [51264/60000]
# loss: 0.513751  [57664/60000]
# Test Error: 
#  Accuracy: 83.4%, Avg loss: 0.563370 
# 
# Epoch 17
# -------------------------------
# loss: 0.683864  [   64/60000]
# loss: 0.518488  [ 6464/60000]
# loss: 0.552603  [12864/60000]
# loss: 0.587414  [19264/60000]
# loss: 0.597717  [25664/60000]
# loss: 0.455639  [32064/60000]
# loss: 0.580606  [38464/60000]
# loss: 0.575656  [44864/60000]
# loss: 0.441984  [51264/60000]
# loss: 0.502875  [57664/60000]
# Test Error: 
#  Accuracy: 84.9%, Avg loss: 0.512437 
# 
# Epoch 18
# -------------------------------
# loss: 0.475052  [   64/60000]
# loss: 0.513206  [ 6464/60000]
# loss: 0.475608  [12864/60000]
# loss: 0.472173  [19264/60000]
# loss: 0.565198  [25664/60000]
# loss: 0.561736  [32064/60000]
# loss: 0.411693  [38464/60000]
# loss: 0.388054  [44864/60000]
# loss: 0.354307  [51264/60000]
# loss: 0.577158  [57664/60000]
# Test Error: 
#  Accuracy: 86.5%, Avg loss: 0.466518 
# 
# Epoch 19
# -------------------------------
# loss: 0.484249  [   64/60000]
# loss: 0.800616  [ 6464/60000]
# loss: 0.648874  [12864/60000]
# loss: 0.370886  [19264/60000]
# loss: 0.442482  [25664/60000]
# loss: 0.543971  [32064/60000]
# loss: 0.465536  [38464/60000]
# loss: 0.386965  [44864/60000]
# loss: 0.344756  [51264/60000]
# loss: 0.235041  [57664/60000]
# Test Error: 
#  Accuracy: 87.1%, Avg loss: 0.434546 
# 
# Epoch 20
# -------------------------------
# loss: 0.342843  [   64/60000]
# loss: 0.573997  [ 6464/60000]
# loss: 0.348703  [12864/60000]
# loss: 0.485653  [19264/60000]
# loss: 0.408712  [25664/60000]
# loss: 0.296957  [32064/60000]
# loss: 0.434818  [38464/60000]
# loss: 0.455127  [44864/60000]
# loss: 0.335877  [51264/60000]
# loss: 0.286010  [57664/60000]
# Test Error: 
#  Accuracy: 88.0%, Avg loss: 0.409662 
# 
# Epoch 21
# -------------------------------
# loss: 0.369448  [   64/60000]
# loss: 0.430793  [ 6464/60000]
# loss: 0.451369  [12864/60000]
# loss: 0.294245  [19264/60000]
# loss: 0.358771  [25664/60000]
# loss: 0.584439  [32064/60000]
# loss: 0.410982  [38464/60000]
# loss: 0.423105  [44864/60000]
# loss: 0.400600  [51264/60000]
# loss: 0.505432  [57664/60000]
# Test Error: 
#  Accuracy: 88.7%, Avg loss: 0.382053 
# 
# Epoch 22
# -------------------------------
# loss: 0.403954  [   64/60000]
# loss: 0.238183  [ 6464/60000]
# loss: 0.378281  [12864/60000]
# loss: 0.244759  [19264/60000]
# loss: 0.318928  [25664/60000]
# loss: 0.432114  [32064/60000]
# loss: 0.436649  [38464/60000]
# loss: 0.364359  [44864/60000]
# loss: 0.565456  [51264/60000]
# loss: 0.345300  [57664/60000]
# Test Error: 
#  Accuracy: 89.1%, Avg loss: 0.370084 
# 
# Epoch 23
# -------------------------------
# loss: 0.524268  [   64/60000]
# loss: 0.330927  [ 6464/60000]
# loss: 0.304676  [12864/60000]
# loss: 0.268494  [19264/60000]
# loss: 0.433609  [25664/60000]
# loss: 0.293246  [32064/60000]
# loss: 0.311919  [38464/60000]
# loss: 0.554731  [44864/60000]
# loss: 0.303064  [51264/60000]
# loss: 0.237600  [57664/60000]
# Test Error: 
#  Accuracy: 89.6%, Avg loss: 0.346659 
# 
# Epoch 24
# -------------------------------
# loss: 0.496005  [   64/60000]
# loss: 0.536406  [ 6464/60000]
# loss: 0.276415  [12864/60000]
# loss: 0.300939  [19264/60000]
# loss: 0.243768  [25664/60000]
# loss: 0.308775  [32064/60000]
# loss: 0.361699  [38464/60000]
# loss: 0.232909  [44864/60000]
# loss: 0.523727  [51264/60000]
# loss: 0.304411  [57664/60000]
# Test Error: 
#  Accuracy: 90.0%, Avg loss: 0.335268 
# 
# Epoch 25
# -------------------------------
# loss: 0.345141  [   64/60000]
# loss: 0.326243  [ 6464/60000]
# loss: 0.512958  [12864/60000]
# loss: 0.329920  [19264/60000]
# loss: 0.368344  [25664/60000]
# loss: 0.361410  [32064/60000]
# loss: 0.228962  [38464/60000]
# loss: 0.351515  [44864/60000]
# loss: 0.307803  [51264/60000]
# loss: 0.487760  [57664/60000]
# Test Error: 
#  Accuracy: 90.5%, Avg loss: 0.319166 
# 
# Epoch 26
# -------------------------------
# loss: 0.529393  [   64/60000]
# loss: 0.508758  [ 6464/60000]
# loss: 0.215395  [12864/60000]
# loss: 0.319885  [19264/60000]
# loss: 0.411065  [25664/60000]
# loss: 0.417220  [32064/60000]
# loss: 0.465183  [38464/60000]
# loss: 0.403677  [44864/60000]
# loss: 0.341376  [51264/60000]
# loss: 0.336558  [57664/60000]
# Test Error: 
#  Accuracy: 90.5%, Avg loss: 0.312447 
# 
# Epoch 27
# -------------------------------
# loss: 0.456818  [   64/60000]
# loss: 0.476977  [ 6464/60000]
# loss: 0.207428  [12864/60000]
# loss: 0.263893  [19264/60000]
# loss: 0.284180  [25664/60000]
# loss: 0.230008  [32064/60000]
# loss: 0.321062  [38464/60000]
# loss: 0.207717  [44864/60000]
# loss: 0.274002  [51264/60000]
# loss: 0.382409  [57664/60000]
# Test Error: 
#  Accuracy: 90.8%, Avg loss: 0.301300 
# 
# Epoch 28
# -------------------------------
# loss: 0.251662  [   64/60000]
# loss: 0.367155  [ 6464/60000]
# loss: 0.297113  [12864/60000]
# loss: 0.239389  [19264/60000]
# loss: 0.460457  [25664/60000]
# loss: 0.303222  [32064/60000]
# loss: 0.419137  [38464/60000]
# loss: 0.198897  [44864/60000]
# loss: 0.324739  [51264/60000]
# loss: 0.316162  [57664/60000]
# Test Error: 
#  Accuracy: 91.3%, Avg loss: 0.286547 
# 
# Epoch 29
# -------------------------------
# loss: 0.327305  [   64/60000]
# loss: 0.141737  [ 6464/60000]
# loss: 0.343407  [12864/60000]
# loss: 0.162212  [19264/60000]
# loss: 0.288561  [25664/60000]
# loss: 0.329266  [32064/60000]
# loss: 0.162561  [38464/60000]
# loss: 0.501124  [44864/60000]
# loss: 0.136634  [51264/60000]
# loss: 0.320445  [57664/60000]
# Test Error: 
#  Accuracy: 91.9%, Avg loss: 0.276883 
# 
# Epoch 30
# -------------------------------
# loss: 0.200224  [   64/60000]
# loss: 0.134913  [ 6464/60000]
# loss: 0.315140  [12864/60000]
# loss: 0.260893  [19264/60000]
# loss: 0.306863  [25664/60000]
# loss: 0.304458  [32064/60000]
# loss: 0.282107  [38464/60000]
# loss: 0.286806  [44864/60000]
# loss: 0.282757  [51264/60000]
# loss: 0.324279  [57664/60000]
# Test Error: 
#  Accuracy: 91.9%, Avg loss: 0.268497 
# 
# Epoch 31
# -------------------------------
# loss: 0.245400  [   64/60000]
# loss: 0.289963  [ 6464/60000]
# loss: 0.324431  [12864/60000]
# loss: 0.288454  [19264/60000]
# loss: 0.357657  [25664/60000]
# loss: 0.363224  [32064/60000]
# loss: 0.387493  [38464/60000]
# loss: 0.429391  [44864/60000]
# loss: 0.222655  [51264/60000]
# loss: 0.206475  [57664/60000]
# Test Error: 
#  Accuracy: 92.2%, Avg loss: 0.262875 
# 
# Epoch 32
# -------------------------------
# loss: 0.139992  [   64/60000]
# loss: 0.302331  [ 6464/60000]
# loss: 0.199107  [12864/60000]
# loss: 0.153804  [19264/60000]
# loss: 0.281483  [25664/60000]
# loss: 0.386757  [32064/60000]
# loss: 0.102005  [38464/60000]
# loss: 0.337656  [44864/60000]
# loss: 0.337839  [51264/60000]
# loss: 0.224534  [57664/60000]
# Test Error: 
#  Accuracy: 92.5%, Avg loss: 0.253550 
# 
# Epoch 33
# -------------------------------
# loss: 0.459032  [   64/60000]
# loss: 0.178409  [ 6464/60000]
# loss: 0.262921  [12864/60000]
# loss: 0.237601  [19264/60000]
# loss: 0.208098  [25664/60000]
# loss: 0.252299  [32064/60000]
# loss: 0.194583  [38464/60000]
# loss: 0.358626  [44864/60000]
# loss: 0.282218  [51264/60000]
# loss: 0.175461  [57664/60000]
# Test Error: 
#  Accuracy: 92.5%, Avg loss: 0.251536 
# 
# Epoch 34
# -------------------------------
# loss: 0.263093  [   64/60000]
# loss: 0.199493  [ 6464/60000]
# loss: 0.182696  [12864/60000]
# loss: 0.188761  [19264/60000]
# loss: 0.307002  [25664/60000]
# loss: 0.134379  [32064/60000]
# loss: 0.324733  [38464/60000]
# loss: 0.161165  [44864/60000]
# loss: 0.285761  [51264/60000]
# loss: 0.198336  [57664/60000]
# Test Error: 
#  Accuracy: 93.0%, Avg loss: 0.240541 
# 
# Epoch 35
# -------------------------------
# loss: 0.270362  [   64/60000]
# loss: 0.312266  [ 6464/60000]
# loss: 0.059151  [12864/60000]
# loss: 0.242004  [19264/60000]
# loss: 0.165756  [25664/60000]
# loss: 0.207548  [32064/60000]
# loss: 0.310522  [38464/60000]
# loss: 0.254762  [44864/60000]
# loss: 0.137832  [51264/60000]
# loss: 0.221974  [57664/60000]
# Test Error: 
#  Accuracy: 93.0%, Avg loss: 0.234484 
# 
# Epoch 36
# -------------------------------
# loss: 0.395438  [   64/60000]
# loss: 0.158618  [ 6464/60000]
# loss: 0.244143  [12864/60000]
# loss: 0.156863  [19264/60000]
# loss: 0.172943  [25664/60000]
# loss: 0.362151  [32064/60000]
# loss: 0.512281  [38464/60000]
# loss: 0.238284  [44864/60000]
# loss: 0.302593  [51264/60000]
# loss: 0.249378  [57664/60000]
# Test Error: 
#  Accuracy: 93.4%, Avg loss: 0.227789 
# 
# Epoch 37
# -------------------------------
# loss: 0.082197  [   64/60000]
# loss: 0.354847  [ 6464/60000]
# loss: 0.183657  [12864/60000]
# loss: 0.249841  [19264/60000]
# loss: 0.205619  [25664/60000]
# loss: 0.218282  [32064/60000]
# loss: 0.347050  [38464/60000]
# loss: 0.155064  [44864/60000]
# loss: 0.147061  [51264/60000]
# loss: 0.359574  [57664/60000]
# Test Error: 
#  Accuracy: 93.3%, Avg loss: 0.223504 
# 
# Epoch 38
# -------------------------------
# loss: 0.168005  [   64/60000]
# loss: 0.221372  [ 6464/60000]
# loss: 0.133618  [12864/60000]
# loss: 0.356000  [19264/60000]
# loss: 0.189959  [25664/60000]
# loss: 0.345857  [32064/60000]
# loss: 0.150549  [38464/60000]
# loss: 0.196183  [44864/60000]
# loss: 0.196592  [51264/60000]
# loss: 0.282022  [57664/60000]
# Test Error: 
#  Accuracy: 93.7%, Avg loss: 0.219208 
# 
# Epoch 39
# -------------------------------
# loss: 0.272736  [   64/60000]
# loss: 0.231616  [ 6464/60000]
# loss: 0.457999  [12864/60000]
# loss: 0.358371  [19264/60000]
# loss: 0.502934  [25664/60000]
# loss: 0.192713  [32064/60000]
# loss: 0.084918  [38464/60000]
# loss: 0.472445  [44864/60000]
# loss: 0.261560  [51264/60000]
# loss: 0.266010  [57664/60000]
# Test Error: 
#  Accuracy: 93.7%, Avg loss: 0.214224 
# 
# Epoch 40
# -------------------------------
# loss: 0.261929  [   64/60000]
# loss: 0.650823  [ 6464/60000]
# loss: 0.164772  [12864/60000]
# loss: 0.229401  [19264/60000]
# loss: 0.236858  [25664/60000]
# loss: 0.309083  [32064/60000]
# loss: 0.215060  [38464/60000]
# loss: 0.127985  [44864/60000]
# loss: 0.232234  [51264/60000]
# loss: 0.299142  [57664/60000]
# Test Error: 
#  Accuracy: 93.6%, Avg loss: 0.212960 
# 
# Done!
# =============================================================================
